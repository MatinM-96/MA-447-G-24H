{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Objective function G(v)\n",
    "def G(v):\n",
    "    Q = np.array([[1, 3], [5, 7]])  # Quadratic matrix\n",
    "    r = np.array([1, 1])  # Linear term\n",
    "    c = 2  # Constant term\n",
    "    return np.einsum('ij,ij->i', v @ Q, v) + r @ v.T + c\n",
    "\n",
    "# Constraint function c1(v) = 2*v1 - 5*v2 - 2\n",
    "def c1(v):\n",
    "    return np.array([2 * v[0] - 5 * v[1] - 2])\n",
    "\n",
    "# Constraint function c2(v) = v1 + v2 - 1\n",
    "def c2(v):\n",
    "    return np.array([v[0] + v[1] - 1])\n",
    "\n",
    "# Lagrangian function\n",
    "def lagrangian(v, *lmbda):\n",
    "    l1, l2 = lmbda\n",
    "    return G(v) - l1 * c1(v) - l2 * c2(v)\n",
    "\n",
    "# Lagrangian gradient with respect to v\n",
    "def lagrangian_grad(v, l1, l2):\n",
    "    Q = np.array([[1, 3], [5, 7]])\n",
    "    r = np.array([1, 1])\n",
    "    return 2 * Q @ v + r - 2 * l1 * np.array([2, -5]) - l2 * np.array([1, 1])\n",
    "\n",
    "# Initial guess for decision variables and Lagrange multipliers\n",
    "initial_guess = np.array([0.5, 0.5, 1, 1])\n",
    "\n",
    "# Function to solve system equations (gradient + constraints)\n",
    "def eq_system(vars):\n",
    "    v = vars[:2]  # v1, v2\n",
    "    l1, l2 = vars[2:]  # Lagrange multipliers\n",
    "    return np.concatenate((lagrangian_grad(v, l1, l2), c1(v), c2(v)))\n",
    "\n",
    "# Solve using fsolve\n",
    "solution = fsolve(eq_system, initial_guess)\n",
    "\n",
    "# Extract solution values\n",
    "v_opt = solution[:2]  # Optimal v1, v2\n",
    "l1_opt, l2_opt = solution[2], solution[3]  # Optimal multipliers\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal v1, v2:\", v_opt)\n",
    "print(\"Optimal lambda1:\", l1_opt)\n",
    "print(\"Optimal lambda2:\", l2_opt)\n",
    "\n",
    "# Set up grid for plotting\n",
    "v1_vals = np.linspace(-2, 2, 100)\n",
    "v2_vals = np.linspace(-2, 2, 100)\n",
    "V1, V2 = np.meshgrid(v1_vals, v2_vals)\n",
    "\n",
    "# Compute G(v) over grid for plotting\n",
    "G_vals = G(np.array([V1.flatten(), V2.flatten()]).T).reshape(V1.shape)\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(V1, V2, G_vals, cmap='viridis', alpha=0.6)\n",
    "ax.scatter(v_opt[0], v_opt[1], G(np.array([v_opt]))[0], color='red')  # Optimal point\n",
    "\n",
    "# Plot constraint surfaces in 3D\n",
    "ax.plot_surface(V1, V2, 2 * V1 - 5 * V2 - 2, color='blue', alpha=0.3)\n",
    "ax.plot_surface(V1, V2, V1 + V2 - 1, color='orange', alpha=0.3)\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel('v1')\n",
    "ax.set_ylabel('v2')\n",
    "ax.set_zlabel('G(v)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "transformations = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.view(-1))])\n",
    "fashion_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transformations)\n",
    "\n",
    "# Convert the dataset into a numpy array\n",
    "features = torch.stack([img for img, _ in fashion_data]).numpy()\n",
    "\n",
    "# Normalize the data before PCA\n",
    "data_scaler = StandardScaler()\n",
    "normalized_features = data_scaler.fit_transform(features)\n",
    "\n",
    "# Perform PCA on the normalized data\n",
    "pca_model = PCA(n_components=40)\n",
    "pca_model.fit(normalized_features)\n",
    "\n",
    "# Compute cumulative variance for each component\n",
    "cum_variance = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components that explain at least 50% of the variance\n",
    "num_components_50_var = np.argmax(cum_variance >= 0.50) + 1\n",
    "\n",
    "# Display the number of components for 50% variance\n",
    "print(f\"Number of components to reach 50% variance: {num_components_50_var}\")\n",
    "\n",
    "# Plot cumulative variance for the first 40 components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(1, 41), cum_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.50, color='r', linestyle='-', label='50% variance')\n",
    "plt.axvline(x=num_components_50_var, color='g', linestyle='-', label=f'{num_components_50_var} components')\n",
    "plt.title('Cumulative Explained Variance for First 40 PCA Components')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
